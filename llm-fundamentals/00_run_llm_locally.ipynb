{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e86168e",
   "metadata": {},
   "source": [
    "# Running Inference Locally"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb0482d",
   "metadata": {},
   "source": [
    "Large Language Models (LLMs) have revolutionized AI applications, but they don't always need to be accessed through cloud APIs. \n",
    "\n",
    "In this notebook I download, save and run an LLM locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6764b62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM,AutoTokenizer\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e4914f",
   "metadata": {},
   "source": [
    "### Running LLMs locally offers several advantages:\n",
    "\n",
    "- **Privacy** : Your data doesn't leave your environment\n",
    "- **Cost** : No per-token API charges\n",
    "- **Latency** : No network delays\n",
    "- **Customization** : Full control over model parameters\n",
    "\n",
    "### However, local LLMs also have limitations:\n",
    "\n",
    "- **Hardware requirements** : Models need sufficient RAM and GPU\n",
    "- **Model size** : Smaller models fit locally but may have reduced capabilities\n",
    "- **Updates** : You manage model versions yourself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f1ecdda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading model from Hugging Face Hub...\n",
      "\n",
      "Model name : distilgpt2\n",
      "Number of parameters : 81912576\n",
      "Model size on disk : 312.47 MB (estimated)\n",
      "\n",
      "Saving model to : ./downloaded_llms/distilgpt2_model\n",
      "model and tokenizer saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Set the directory to save the model\n",
    "save_directory = \"./downloaded_llms/distilgpt2_model\"\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(save_directory, exist_ok=True)\n",
    "\n",
    "# Load model and tokenizer\n",
    "print(\"Downloading model from Hugging Face Hub...\")\n",
    "model_name = \"distilgpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Print model information\n",
    "print()\n",
    "print(f\"Model name : {model_name}\")\n",
    "print(f\"Number of parameters : {model.num_parameters()}\")\n",
    "print(f\"Model size on disk : {model.num_parameters() * 4 / (1024 * 1024):.2f} MB (estimated)\")\n",
    "print()\n",
    "\n",
    "# Save the model and tokenizer to the specified directory\n",
    "print(f\"Saving model to : {save_directory}\")\n",
    "model.save_pretrained(save_directory)\n",
    "tokenizer.save_pretrained(save_directory)\n",
    "print(\"model and tokenizer saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6ae13a",
   "metadata": {},
   "source": [
    "## Loading and Using a Local Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b4ae20",
   "metadata": {},
   "source": [
    "Once the model is saved, it can be loaded from local storage instead of downloading it again. \n",
    "\n",
    "This is especially useful for larger models or when working in environments with limited internet access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "736c9f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from local directory...\n",
      "Model loaded from local directory !\n"
     ]
    }
   ],
   "source": [
    "# Load model from local directory\n",
    "print(\"Loading model from local directory...\")\n",
    "local_model = AutoModelForCausalLM.from_pretrained(save_directory)\n",
    "local_tokenizer = AutoTokenizer.from_pretrained(save_directory)\n",
    "local_tokenizer.pad_token = local_tokenizer.eos_token\n",
    "print(\"Model loaded from local directory !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d78f6c",
   "metadata": {},
   "source": [
    "## Generating Text with a Local LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8bf606",
   "metadata": {},
   "source": [
    "This is text generation function that allows to control various parameters :\n",
    "\n",
    "- **Temperature** : Controls randomness (higher = more creative, lower = more deterministic)\n",
    "- **Max length** : The maximum number of tokens to generate\n",
    "- **Top-p (nucleus sampling)** : Limits token selection to a subset of most likely tokens\n",
    "- **Top-k** : Limits selection to the k most likely tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b889842f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt, max_length=50, temperature=.8, top_p=.9, top_k=50, do_sample=True):\n",
    "    \"\"\"\n",
    "        Generate text from a prompt with custom parameters\n",
    "        \n",
    "        Args:\n",
    "            prompt (str): The input text to continue\n",
    "            max_length (int): Maximum length of generated text (including prompt)\n",
    "            temperature (float): Higher values (>1.0) increase randomness, lower values (<1.0) make it more deterministic\n",
    "            top_p (float): Nucleus sampling parameter (0-1.0)\n",
    "            top_k (int): Limits selection to k most likely tokens\n",
    "            do_sample (bool): If False, uses greedy decoding instead of sampling\n",
    "            \n",
    "        Returns:\n",
    "            str: The generated text including the prompt\n",
    "    \"\"\"\n",
    "    \n",
    "    device = \"cpu\"\n",
    "    # Prepare the inputs : str to tokens\n",
    "    inputs = local_tokenizer(prompt, return_tensors=\"pt\", return_attention_mask=True).to(device)\n",
    "    \n",
    "    # Generate text\n",
    "    output = local_model.generate(\n",
    "        input_ids=inputs.input_ids,\n",
    "        attention_mask=inputs.attention_mask,\n",
    "        max_length=max_length,\n",
    "        do_sample=do_sample,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        top_k=top_k,\n",
    "        pad_token_id=local_tokenizer.pad_token_id\n",
    "    )\n",
    "    \n",
    "    # Decode the output\n",
    "    generated_text = local_tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Clean excess whitespace \n",
    "    clean_text = re.sub(r\"\\s+\", \" \", generated_text)\n",
    "    \n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f988c42f",
   "metadata": {},
   "source": [
    "### Experimenting with Different Generation Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "82fc7c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1: Default parameters (temperature=0.8)\n",
      "Prompt: \"Welcome to Fundamentals of LLM Engineering course. This class\"\n",
      "Generated: Welcome to Fundamentals of LLM Engineering course. This class will take a very different approach to LLM engineering. The main purpose of this course is to give students a foundation to do the following things: 1) to use LLM tools to build the LLM framework. This is the best way to build the LLM framework, with the best\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Example 2: Low temperature (more deterministic)\n",
      "Prompt: \"Welcome to Fundamentals of LLM Engineering course. This class\"\n",
      "Generated: Welcome to Fundamentals of LLM Engineering course. This class is designed to help you understand the fundamentals of LLM engineering. \n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Example 3: High temperature (more creative/random)\n",
      "Prompt: \"Welcome to Fundamentals of LLM Engineering course. This class\"\n",
      "Generated: Welcome to Fundamentals of LLM Engineering course. This class explores the basic aspects of LLM, how it defines interdependent theory within it, how it addresses the problems encountered as applied to the LLM application and, whether new ones work when the new ideas for new designs change at all. See the following two course courses and courses on LLM engineering: LNP\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Example 4: Greedy decoding (no sampling, always selects most likely token)\n",
      "Prompt: \"Welcome to Fundamentals of LLM Engineering course. This class\"\n",
      "Generated: Welcome to Fundamentals of LLM Engineering course. This class is designed to help you understand the fundamentals of LLM engineering. \n"
     ]
    }
   ],
   "source": [
    "prompt = \"Welcome to Fundamentals of LLM Engineering course. This class\"\n",
    "\n",
    "print(\"Example 1: Default parameters (temperature=0.8)\")\n",
    "print(f\"Prompt: \\\"{prompt}\\\"\")\n",
    "print(f\"Generated: {generate_text(prompt, max_length=75)}\")\n",
    "print(\"-\"*100)\n",
    "print()\n",
    "\n",
    "print(\"Example 2: Low temperature (more deterministic)\")\n",
    "print(f\"Prompt: \\\"{prompt}\\\"\")\n",
    "print(f\"Generated: {generate_text(prompt, temperature=0.2, max_length=75)}\")\n",
    "print(\"-\"*100)\n",
    "print()\n",
    "\n",
    "print(\"Example 3: High temperature (more creative/random)\")\n",
    "print(f\"Prompt: \\\"{prompt}\\\"\")\n",
    "print(f\"Generated: {generate_text(prompt, temperature=1.5, max_length=75)}\")\n",
    "print(\"-\"*100)\n",
    "print()\n",
    "\n",
    "print(\"Example 4: Greedy decoding (no sampling, always selects most likely token)\")\n",
    "print(f\"Prompt: \\\"{prompt}\\\"\")\n",
    "print(f\"Generated: {generate_text(prompt, top_p=None, temperature=None, do_sample=False, max_length=75)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml-learning-lab)",
   "language": "python",
   "name": "ml-learning-lab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
